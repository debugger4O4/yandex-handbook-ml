# Обучение генеративной трансформерной модели с помощью transformers

В этой работе мы познакомимся на практике с процессом тренировки большой трансформерной языковой модели. Поскольку такая тренировка требует существенных вычислительных ресурсов, выполнять эту работу рекомендуется в Yandex DataSphere, в которой доступны вычислительные узлы с одним или двумя графическими процессорами Tesla V100.

## Архитектура трансформеров

В рамках этой работы мы предполагаем, что вы уже знакомы с архитектурой трансформеров, например, по статье из ML-хэндбука. Также для первоначального знакомства рекомендую заметку Jay Alammar. The Illustrated Transformer, и её частичный русскоязычный перевод.

Мы не будем в рамках работы создавать архитектуру нейросети "с нуля". Если вам интересно изучить реализацию трансформеров - рекомендую посмотреть на NanoGPT. Подробно эта реализация разбирается в этом видео.

## Библиотека transformers и её друзья

Стандартом де-факто в реализации трансформеров служит библиотека transformers от HuggingFace. Она содержит в себе реализацию большого количества используемых трансформерных архитектур, а также ряд полезных инструментов для их обучения. Многие инструменты также оформлены в виде отдельных библиотек, которые хорошо работают вместе:

* **tokenizers** - быстрая реализация различных токенизаторов, позволяющих разделять входной текст на токены
* **datasets** - манипулирование большими датасетами
* **evaluate** - вычисление различных метрик и оценка результатов обучения
* **accelerate** - реализация вычислений на множестве GPU и на вычислительных кластерах

Для начала, установим необходимые библиотеки:

```python
%pip install transformers tokenizers datasets evaluate accelerate
```

Установив нужные пакеты, приступим к следующему этапу работы.

## Подготовка датасета

В нашем примере, мы будем обучать виртуального Льва Толстого. Для этого, возьмем все основные романы писателя, и подготовим из них датасет. В качестве отправной точки будет использовать тексты из библиотеки Мошкова. Соберем ссылки на романы Анна Каренина, Война и мир и др. в один список:

```python
urls = [
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0039.shtml",
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0040.shtml",
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0050.shtml",
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0060.shtml",
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0070.shtml",
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0080.shtml",
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0090.shtml",
    "http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_1860_dekabristy.shtml",
]
```

Теперь скачаем все материалы и подготовим из них один большой текстовый файл. Для того нам понадобится убрать HTML-теги, а также несколько начальных строк в каждом из файлов.

```python
import html
import re
import requests

def download(url):
    return requests.get(url).text

striptags_re = re.compile(r"(<!--.*?-->|<[^>]*>)")
entity_re = re.compile(r"&([^;]+);")

def to_text(s):
    return html.unescape(striptags_re.sub("", s))

def beautify(s):
    lines = [x.strip() for x in s.split("\n") if x.strip() != ""]
    for i in range(min(100, len(lines))):
        if lines[i] == "-->":
            break
    return "\n".join(lines[i + 1:] if i < 100 else lines)

with open("dataset.txt", "w", encoding="utf-8") as f:
    for u in urls:
        text = beautify(to_text(download(u)))
        f.write(text + "\n\n")
```

В результате мы получили один большой файл `dataset.txt`, содержащий большой корпус текстов Льва Толстого.

## Токенизация

Нейросети работают с числами, поэтому первым этапом является токенизация текста, т.е. разбиение его на атомарные элементы, которые затем можно добавить в словарь, и представлять текст как последовательность индексов в словаре. Текст можно токенизировать по буквам, или по словам.

При построении современных генеративных сетей текст обычно разбивают на фрагменты таким образом, чтобы частота появления каждого фрагмента в тексте была примерно одинакова. Это лежит в основе т.н. Byte-Pair Encoding (BPE). Подробнее можно прочитать в этой статье.

Для обучения своего токенизатора используем библиотеку tokenizers:

```python
import tokenizers as tok
import transformers as tr

tokenizer = tok.Tokenizer(tok.models.BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = tok.pre_tokenizers.Whitespace()
trainer = tok.trainers.BpeTrainer(special_tokens=["[PAD]"])
tokenizer.train(["dataset.txt"], trainer)
tokenizer.enable_padding()
```

В данном случае мы используем два специальных токена - `[UNK]` для представления неизвестного токена (такое случится, если на вход попадёт символ, который токенизатор не видел при обучении), и `[PAD]` для паддинга - он используется, если нужно дополнить последовательность до определенной длины.

Вот как можно закодировать входной текст:

```python
tokenizer.encode("Иван Сигизмундович подошел к окну и закашлялся. Вечерело.").tokens
```

Выход:

```python
['Иван', 'С', 'иг', 'изу', 'н', 'до', 'вич', 'подошел', 'к', 'окну', 'и', 'закашлялся', '.', 'Вечер', 'ело', '.']
```

Видим, что популярные слова токенизируются целиком, а те, которые встречаются в тексте редко или не встречаются вовсе - разбиваются на фрагменты.

## Генеративные трансформеры

Для генерации текста используются архитектуры GPT - Generative Pre-trained Transformers. В то время как полноценные трансформеры являются энкодер-декодерной архитектурой, т.е. могут решать задачи преобразования одного вида последовательности в другую, GPT является только декодером, т.к. способен прогнозировать распределение вероятности следующего слова по начальной части последовательности.

Мы используем архитектуру GPT-2, которая, с одной стороны, не слишком велика, а с другой - может неплохо обучиться. Сначала попробуем натренировать такую архитектуру "с нуля".

Сначала нам потребуется преобразовать наш токенизатор к объекту `ttokenizer`, который понимает библиотека transformers.

```python
vocab = tokenizer.get_vocab()
ttokenizer = tr.PreTrainedTokenizerFast(tokenizer_object=tokenizer)
len(vocab)
```

Создаем непосредственно нейросетевую модель GPT2. При этом основные параметры (количество слоев, количество голов внимания и т.д.) оставляем по умолчанию.

```python
config = tr.GPT2Config(
    vocab_size=len(vocab),
    bos_token_id=tokenizer.token_to_id("[CLS]"),
    eos_token_id=tokenizer.token_to_id("[EOS]")
)
gpt = tr.GPT2LMHeadModel(config)
```

Теперь мы готовы приступить к тренировке нашей модели.