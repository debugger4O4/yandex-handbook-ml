## Линейные модели

**Линейные модели от линейной до логистической регрессии. Регуляризация, работа с категориальными признаками, многоклассовая 
классификация**

Мы начнем с самых простых и понятных моделей машинного обучения: линейных. В этом параграфе мы разберёмся, что это такое,
почему они работают и в каких случаях их стоит использовать. Так как это первый класс моделей, с которым вы столкнётесь,
мы постараемся подробно проговорить все важные моменты. Заодно объясним, как работает машинное обучение, на сравнительно
простых примерах.

### Почему модели линейные?

Представьте, что у вас есть множество объектов `X`, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру,
у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы
разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится 
простейшая задача классификации. Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели 
бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру,
оценивать потенциальную годовую доходность шахты. Это пример задачи регрессии. Числа, которым мы хотим сопоставить объекты
из нашего множества иногда называют таргетами (от английского **target**).

Таким образом, задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов `X` в 
множество возможных таргетов.

Математически задачи можно описать так:

- **классификация**: `X → {0, 1, …, K}`, где `0, …, K` – номера классов,
- **регрессия**: `X → R`.

Очевидно, что просто сопоставить какие-то объекты каким-то числам — дело довольно бессмысленное. Мы же хотим быстро 
обнаруживать мошенников или принимать решение, где строить шахту. Значит нам нужен какой-то критерий качества. Мы бы 
хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что 
значит «лучше всего» – вопрос сложный. Мы к нему будем много раз возвращаться. Однако, есть более простой вопрос: среди 
каких отображений мы будем искать самое лучшее? Возможных отображений может быть много, но мы можем упростить себе задачу
и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Весь
этот параграф будет посвящен самому простому такому семейству — линейным функциям вида

![img.png](content%2Fimg.png),

где `y` – целевая переменная (**таргет**), `(x1, ..., xD)`– вектор, соответствующий объекту выборки (**вектор признаков**), а
`w1, ..., wD, w0` – параметры модели. Признаки ещё называют **фичами** (от английского **features**). Вектор `w = (w1, ..., wD)`
часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта, а число
– свободным коэффициентом, или **сдвигом** (**bias**). Более компактно линейную модель можно записать в виде

![img_1.png](content%2Fimg_1.png)

Теперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем
не какое-то абстрактное отображение, а конкретный вектор `(w0, w1, ..., wD) ∈ R^D+1`

Замечание. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков `x1, ..., xD`. 
Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель 
называют линейной, если она является линейной по этим численным признакам.

Разберёмся, как будет работать такая модель в случае, если `D = 1`. То есть у наших объектов есть ровно один численный 
признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: `y = w1x1 + w0`. Для задачи
регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить 
линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по картам. Допустим, 
нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально
мошеннические мы будем искать так называемое разделяющее правило: там, где значение функции положительно, мы будем предсказывать
один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций,
после которого есть смысл пометить транзакцию как подозрительную.

![img_2.png](content%2Fimg_2.png)

В случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом.

**Вопрос на подумать**. Если вы посмотрите содержание учебника, то не найдёте в нём ни «полиномиальных» моделей, ни каких-нибудь
«логарифмических», хотя, казалось бы, зависимости бывают довольно сложными. Почему так?

Линейные зависимости не так просты, как кажется. Пусть мы решаем задачу регрессии. Если мы подозреваем, что целевая переменная
`y` не выражается через `x1`, `x2` как линейная функция, а зависит ещё от логарифма `x1` и ещё как-нибудь от того, разные
ли знаки у признаков, то мы можем ввести дополнительные слагаемые в нашу линейную зависимость, просто объявим эти слагаемые
новыми переменными и добавив перед ними соответствующие регрессионные коэффициенты

![img_3.png](content%2Fimg_3.png)

**Вопрос на подумать**. А как быть, если одна из фичей является категориальной, то есть принимает значения из (обычно конечного числа)
значений, не являющихся числами? Например, это может быть время года, уровень образования, марка машины и так далее. 
Как правило, с такими значениями невозможно производить арифметические операции или же результаты их применения не имеют смысла.

В линейную модель можно подать только численные признаки, так что категориальную фичу придётся как-то закодировать. 
Рассмотрим для примера вот такой датасет

![img_4.png](content%2Fimg_4.png)

Здесь два категориальных признака – `pet_type` и `color`. Первый принимает четыре различных значения, второй – пять.

Самый простой способ – использовать **one-hot кодирование** (**one-hot encoding**). Пусть исходный признак мог принимать `M` значений
`c1, ..., cM`. Давайте заменим категориальный признак на `M` признаков, которые принимают значения `0` и `1`: `i`-й будет
отвечать на вопрос «принимает ли признак значение `ci`?». Иными словами, вместо ячейки со значением `ci`
у объекта появляется строка нулей и единиц, в которой единица стоит только на `i`-м месте.

В нашем примере получится вот такая табличка:

![img_5.png](content%2Fimg_5.png)

Можно было бы на этом остановиться, но добавленные признаки обладают одним неприятным свойством: в каждом из них ровно 
одна единица, так что сумма соответствующих столбцов равна столбцу из единиц. А это уже плохо. Представьте, что у нас есть
линейная модель

![img_6.png](content%2Fimg_6.png)

Преобразуем немного правую часть:

![img_7.png](content%2Fimg_7.png)</br>
![img_8.png](content%2Fimg_8.png)

Как видим, от одного из новых признаков можно избавиться, не меняя модель. Более того, это стоит сделать, потому что 
наличие «лишних» признаков ведёт к переобучению или вовсе ломает модель – подробнее об этом мы поговорим в разделе про 
регуляризацию. Поэтому при использовании one-hot-encoding обычно выкидывают признак, соответствующий одному из значений. 
Например, в нашем примере итоговая матрица объекты-признаки будет иметь вид:

![img_9.png](content%2Fimg_9.png)

Конечно, one-hot кодирование – это самый наивный способ работы с категориальными признаками, и для более сложных фичей 
или фичей с большим количеством значений оно плохо подходит. С рядом более продвинутых техник вы познакомитесь в разделе 
про обучение представлений.

Помимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют
на результат те или иные признаки. Скажем, если вес `wi` положителен, то с ростом `i`-го признака таргет в случае регрессии 
будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже
имеет прозрачную интерпретацию: чем вес `wi` больше, тем «важнее» `i`-й признак для итогового предсказания. То есть, если
вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют
интерпретируемостью. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели
может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется.
При этом не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или 
градиентного бустинга интерпретировать довольно сложно.

В то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин:
- Линейные модели всё-таки довольно узкий класс функций, они неплохо работают для небольших датасетов и простых задач. 
Однако, если вы решаете линейной моделью более сложную задачу, то вам, скорее всего, придётся выдумывать дополнительные признаки, 
являющиеся сложными функциями от исходных. Поиск таких дополнительных признаков называется **feature engineering**, технически 
он устроен примерно так, как мы описали в вопросе про "полиномиальные модели". Вот только поиском таких искусственных 
фичей можно сильно увлечься, так что осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, 
строившего модель.
- Если между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять
физический смысл (об этой проблеме и о том, как с ней бороться, мы поговорим дальше, когда будем обсуждать регуляризацию).
- Особенно осторожно стоит верить в утверждения вида «этот коэффициент маленький, значит, этот признак не важен». Во-первых,
всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно
может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например,
путём проверки статистического критерия (об этом мы коротко упомянем в разделе про вероятностные модели).
- Конкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку
сходиться к весам «наилучшей» линейной модели, которую можно было бы построить по всем-всем-всем данным на свете.

Обсудив немного общие свойства линейных моделей, перейдём к тому, как их всё-таки обучать. Сначала разберёмся с регрессией, 
а затем настанет черёд классификации.